{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAAlI-24AyWp",
        "outputId": "d755b9f6-2b8f-4cce-d75f-cbfa294d2f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.2.1+cu121)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdiffeq\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchdiffeq-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OkfOHNUvAQ4R",
        "outputId": "73ba04ba-28ac-49e8-c9ca-d0b87dd75d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:/content/drive/MyDrive/Colab Notebooks/cs376.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/cs376.ipynb\n",
            "INFO:root:{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"gpuType\":\"T4\",\"mount_file_id\":\"1vuKQXEBvbzCQZd6LwPIBfOCYpdWFJ7qY\",\"authorship_tag\":\"ABX9TyOx0Tty5k9/m4VL2Po7vGz8\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"},\"accelerator\":\"GPU\"},\"cells\":[{\"cell_type\":\"code\",\"source\":[\"!pip install torchdiffeq\"],\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"GAAlI-24AyWp\",\"outputId\":\"d755b9f6-2b8f-4cce-d75f-cbfa294d2f99\"},\"execution_count\":null,\"outputs\":[{\"output_type\":\"stream\",\"name\":\"stdout\",\"text\":[\"Collecting torchdiffeq\\n\",\"  Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\\n\",\"Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.2.1+cu121)\\n\",\"Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.11.4)\\n\",\"Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.25.2)\\n\",\"Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.14.0)\\n\",\"Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (4.11.0)\\n\",\"Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (1.12)\\n\",\"Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.3)\\n\",\"Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.3)\\n\",\"Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2023.6.0)\\n\",\"Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\\n\",\"Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\\n\",\"Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\\n\",\"Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\\n\",\"Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\\n\",\"Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\\n\",\"Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\\n\",\"Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\\n\",\"Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\\n\",\"Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\\n\",\"Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\\n\",\"Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2.2.0)\\n\",\"Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5.0->torchdiffeq)\\n\",\"  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\\n\",\"\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m21.3/21.3 MB\\u001b[0m \\u001b[31m53.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\"\\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (2.1.5)\\n\",\"Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torchdiffeq) (1.3.0)\\n\",\"Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdiffeq\\n\"]}]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"OkfOHNUvAQ4R\"},\"outputs\":[],\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=160)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment1')\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"args, unknown = parser.parse_known_args()\\n\",\"\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\\n\",\"\\n\",\"\\n\",\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\\n\",\"\\n\",\"\\n\",\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\\n\",\"\\n\",\"\\n\",\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\\n\",\"\\n\",\"\\n\",\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\",\"\\n\",\"\\n\",\"if __name__ == '__main__':\\n\",\"\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath('/content/drive/MyDrive/Colab Notebooks/cs376.ipynb'))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\"]}]}\n",
            "{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"provenance\":[],\"gpuType\":\"T4\",\"mount_file_id\":\"1vuKQXEBvbzCQZd6LwPIBfOCYpdWFJ7qY\",\"authorship_tag\":\"ABX9TyOx0Tty5k9/m4VL2Po7vGz8\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"},\"accelerator\":\"GPU\"},\"cells\":[{\"cell_type\":\"code\",\"source\":[\"!pip install torchdiffeq\"],\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"GAAlI-24AyWp\",\"outputId\":\"d755b9f6-2b8f-4cce-d75f-cbfa294d2f99\"},\"execution_count\":null,\"outputs\":[{\"output_type\":\"stream\",\"name\":\"stdout\",\"text\":[\"Collecting torchdiffeq\\n\",\"  Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\\n\",\"Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (2.2.1+cu121)\\n\",\"Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torchdiffeq) (1.11.4)\\n\",\"Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.25.2)\\n\",\"Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.14.0)\\n\",\"Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (4.11.0)\\n\",\"Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (1.12)\\n\",\"Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.3)\\n\",\"Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.3)\\n\",\"Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2023.6.0)\\n\",\"Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\\n\",\"Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\\n\",\"Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\\n\",\"Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\\n\",\"Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\\n\",\"Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\\n\",\"Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\\n\",\"Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\\n\",\"Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\\n\",\"Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\\n\",\"Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.5.0->torchdiffeq)\\n\",\"  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\\n\",\"Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torchdiffeq) (2.2.0)\\n\",\"Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5.0->torchdiffeq)\\n\",\"  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\\n\",\"\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m21.3/21.3 MB\\u001b[0m \\u001b[31m53.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\"\\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (2.1.5)\\n\",\"Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torchdiffeq) (1.3.0)\\n\",\"Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdiffeq\\n\"]}]},{\"cell_type\":\"code\",\"execution_count\":null,\"metadata\":{\"id\":\"OkfOHNUvAQ4R\"},\"outputs\":[],\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=160)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment1')\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"args, unknown = parser.parse_known_args()\\n\",\"\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\\n\",\"\\n\",\"\\n\",\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\\n\",\"\\n\",\"\\n\",\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\\n\",\"\\n\",\"\\n\",\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\\n\",\"\\n\",\"\\n\",\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\",\"\\n\",\"\\n\",\"if __name__ == '__main__':\\n\",\"\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath('/content/drive/MyDrive/Colab Notebooks/cs376.ipynb'))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\"]}]}\n",
            "INFO:root:Namespace(network='odenet', tol=0.001, adjoint=False, downsampling_method='conv', nepochs=160, data_aug=True, lr=0.1, batch_size=128, test_batch_size=1000, save='./experiment1', debug=False, gpu=0)\n",
            "Namespace(network='odenet', tol=0.001, adjoint=False, downsampling_method='conv', nepochs=160, data_aug=True, lr=0.1, batch_size=128, test_batch_size=1000, save='./experiment1', debug=False, gpu=0)\n",
            "INFO:root:Sequential(\n",
            "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (7): ODEBlock(\n",
            "    (odefunc): ODEfunc(\n",
            "      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv1): ConcatConv2d(\n",
            "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (conv2): ConcatConv2d(\n",
            "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (11): Flatten()\n",
            "  (12): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (7): ODEBlock(\n",
            "    (odefunc): ODEfunc(\n",
            "      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv1): ConcatConv2d(\n",
            "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (conv2): ConcatConv2d(\n",
            "        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "    )\n",
            "  )\n",
            "  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "  (9): ReLU(inplace=True)\n",
            "  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (11): Flatten()\n",
            "  (12): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "INFO:root:Number of parameters: 208266\n",
            "Number of parameters: 208266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6737580.82it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 509433.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 2655968.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to .data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11832626.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to .data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "INFO:root:Epoch 0000 | Time 3.713 (3.713) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.0988 | Test Acc 0.0963\n",
            "Epoch 0000 | Time 3.713 (3.713) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.0988 | Test Acc 0.0963\n",
            "INFO:root:Epoch 0001 | Time 0.267 (0.125) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9768 | Test Acc 0.9774\n",
            "Epoch 0001 | Time 0.267 (0.125) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9768 | Test Acc 0.9774\n",
            "INFO:root:Epoch 0002 | Time 0.272 (0.086) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9882 | Test Acc 0.9892\n",
            "Epoch 0002 | Time 0.272 (0.086) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9882 | Test Acc 0.9892\n",
            "INFO:root:Epoch 0003 | Time 0.278 (0.091) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9799 | Test Acc 0.9788\n",
            "Epoch 0003 | Time 0.278 (0.091) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9799 | Test Acc 0.9788\n",
            "INFO:root:Epoch 0004 | Time 0.427 (0.089) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9911 | Test Acc 0.9923\n",
            "Epoch 0004 | Time 0.427 (0.089) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9911 | Test Acc 0.9923\n",
            "INFO:root:Epoch 0005 | Time 0.302 (0.090) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9925 | Test Acc 0.9903\n",
            "Epoch 0005 | Time 0.302 (0.090) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9925 | Test Acc 0.9903\n",
            "INFO:root:Epoch 0006 | Time 0.281 (0.095) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9919 | Test Acc 0.9922\n",
            "Epoch 0006 | Time 0.281 (0.095) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9919 | Test Acc 0.9922\n",
            "INFO:root:Epoch 0007 | Time 0.350 (0.087) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9939\n",
            "Epoch 0007 | Time 0.350 (0.087) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9939\n",
            "INFO:root:Epoch 0008 | Time 0.276 (0.089) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9937 | Test Acc 0.9924\n",
            "Epoch 0008 | Time 0.276 (0.089) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9937 | Test Acc 0.9924\n",
            "INFO:root:Epoch 0009 | Time 0.277 (0.096) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9947 | Test Acc 0.9927\n",
            "Epoch 0009 | Time 0.277 (0.096) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9947 | Test Acc 0.9927\n",
            "INFO:root:Epoch 0010 | Time 0.271 (0.088) | NFE-F 20.5 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.9956\n",
            "Epoch 0010 | Time 0.271 (0.088) | NFE-F 20.5 | NFE-B 0.0 | Train Acc 0.9969 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0011 | Time 0.275 (0.089) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9936\n",
            "Epoch 0011 | Time 0.275 (0.089) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9936\n",
            "INFO:root:Epoch 0012 | Time 0.428 (0.096) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.9961 | Test Acc 0.9947\n",
            "Epoch 0012 | Time 0.428 (0.096) | NFE-F 20.4 | NFE-B 0.0 | Train Acc 0.9961 | Test Acc 0.9947\n",
            "INFO:root:Epoch 0013 | Time 0.289 (0.087) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9945 | Test Acc 0.9926\n",
            "Epoch 0013 | Time 0.289 (0.087) | NFE-F 20.3 | NFE-B 0.0 | Train Acc 0.9945 | Test Acc 0.9926\n",
            "INFO:root:Epoch 0014 | Time 0.286 (0.092) | NFE-F 20.8 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.9943\n",
            "Epoch 0014 | Time 0.286 (0.092) | NFE-F 20.8 | NFE-B 0.0 | Train Acc 0.9968 | Test Acc 0.9943\n",
            "INFO:root:Epoch 0015 | Time 0.270 (0.098) | NFE-F 22.4 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9932\n",
            "Epoch 0015 | Time 0.270 (0.098) | NFE-F 22.4 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9932\n",
            "INFO:root:Epoch 0016 | Time 0.290 (0.102) | NFE-F 23.7 | NFE-B 0.0 | Train Acc 0.9976 | Test Acc 0.9951\n",
            "Epoch 0016 | Time 0.290 (0.102) | NFE-F 23.7 | NFE-B 0.0 | Train Acc 0.9976 | Test Acc 0.9951\n",
            "INFO:root:Epoch 0017 | Time 0.274 (0.103) | NFE-F 24.0 | NFE-B 0.0 | Train Acc 0.9963 | Test Acc 0.9936\n",
            "Epoch 0017 | Time 0.274 (0.103) | NFE-F 24.0 | NFE-B 0.0 | Train Acc 0.9963 | Test Acc 0.9936\n",
            "INFO:root:Epoch 0018 | Time 0.295 (0.107) | NFE-F 25.3 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9922\n",
            "Epoch 0018 | Time 0.295 (0.107) | NFE-F 25.3 | NFE-B 0.0 | Train Acc 0.9949 | Test Acc 0.9922\n",
            "INFO:root:Epoch 0019 | Time 0.298 (0.102) | NFE-F 24.9 | NFE-B 0.0 | Train Acc 0.9960 | Test Acc 0.9927\n",
            "Epoch 0019 | Time 0.298 (0.102) | NFE-F 24.9 | NFE-B 0.0 | Train Acc 0.9960 | Test Acc 0.9927\n",
            "INFO:root:Epoch 0020 | Time 0.295 (0.101) | NFE-F 24.5 | NFE-B 0.0 | Train Acc 0.9981 | Test Acc 0.9950\n",
            "Epoch 0020 | Time 0.295 (0.101) | NFE-F 24.5 | NFE-B 0.0 | Train Acc 0.9981 | Test Acc 0.9950\n",
            "INFO:root:Epoch 0021 | Time 0.443 (0.105) | NFE-F 25.9 | NFE-B 0.0 | Train Acc 0.9978 | Test Acc 0.9948\n",
            "Epoch 0021 | Time 0.443 (0.105) | NFE-F 25.9 | NFE-B 0.0 | Train Acc 0.9978 | Test Acc 0.9948\n",
            "INFO:root:Epoch 0022 | Time 0.491 (0.113) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9964 | Test Acc 0.9946\n",
            "Epoch 0022 | Time 0.491 (0.113) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9964 | Test Acc 0.9946\n",
            "INFO:root:Epoch 0023 | Time 0.297 (0.109) | NFE-F 25.3 | NFE-B 0.0 | Train Acc 0.9975 | Test Acc 0.9943\n",
            "Epoch 0023 | Time 0.297 (0.109) | NFE-F 25.3 | NFE-B 0.0 | Train Acc 0.9975 | Test Acc 0.9943\n",
            "INFO:root:Epoch 0024 | Time 0.301 (0.104) | NFE-F 25.4 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9957\n",
            "Epoch 0024 | Time 0.301 (0.104) | NFE-F 25.4 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0025 | Time 0.301 (0.102) | NFE-F 25.7 | NFE-B 0.0 | Train Acc 0.9976 | Test Acc 0.9951\n",
            "Epoch 0025 | Time 0.301 (0.102) | NFE-F 25.7 | NFE-B 0.0 | Train Acc 0.9976 | Test Acc 0.9951\n",
            "INFO:root:Epoch 0026 | Time 0.428 (0.111) | NFE-F 25.6 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9963\n",
            "Epoch 0026 | Time 0.428 (0.111) | NFE-F 25.6 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9963\n",
            "INFO:root:Epoch 0027 | Time 0.298 (0.107) | NFE-F 26.1 | NFE-B 0.0 | Train Acc 0.9984 | Test Acc 0.9964\n",
            "Epoch 0027 | Time 0.298 (0.107) | NFE-F 26.1 | NFE-B 0.0 | Train Acc 0.9984 | Test Acc 0.9964\n",
            "INFO:root:Epoch 0028 | Time 0.303 (0.105) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9952\n",
            "Epoch 0028 | Time 0.303 (0.105) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0029 | Time 0.433 (0.104) | NFE-F 25.5 | NFE-B 0.0 | Train Acc 0.9979 | Test Acc 0.9947\n",
            "Epoch 0029 | Time 0.433 (0.104) | NFE-F 25.5 | NFE-B 0.0 | Train Acc 0.9979 | Test Acc 0.9947\n",
            "INFO:root:Epoch 0030 | Time 0.319 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9949\n",
            "Epoch 0030 | Time 0.319 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9949\n",
            "INFO:root:Epoch 0031 | Time 0.301 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9952\n",
            "Epoch 0031 | Time 0.301 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0032 | Time 0.308 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9992 | Test Acc 0.9953\n",
            "Epoch 0032 | Time 0.308 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9992 | Test Acc 0.9953\n",
            "INFO:root:Epoch 0033 | Time 0.395 (0.113) | NFE-F 26.1 | NFE-B 0.0 | Train Acc 0.9979 | Test Acc 0.9949\n",
            "Epoch 0033 | Time 0.395 (0.113) | NFE-F 26.1 | NFE-B 0.0 | Train Acc 0.9979 | Test Acc 0.9949\n",
            "INFO:root:Epoch 0034 | Time 0.294 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9978 | Test Acc 0.9936\n",
            "Epoch 0034 | Time 0.294 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9978 | Test Acc 0.9936\n",
            "INFO:root:Epoch 0035 | Time 0.299 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9963\n",
            "Epoch 0035 | Time 0.299 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9963\n",
            "INFO:root:Epoch 0036 | Time 0.475 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9983 | Test Acc 0.9945\n",
            "Epoch 0036 | Time 0.475 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9983 | Test Acc 0.9945\n",
            "INFO:root:Epoch 0037 | Time 0.318 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9981 | Test Acc 0.9950\n",
            "Epoch 0037 | Time 0.318 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9981 | Test Acc 0.9950\n",
            "INFO:root:Epoch 0038 | Time 0.314 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9950\n",
            "Epoch 0038 | Time 0.314 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9950\n",
            "INFO:root:Epoch 0039 | Time 0.294 (0.103) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9987 | Test Acc 0.9950\n",
            "Epoch 0039 | Time 0.294 (0.103) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.9987 | Test Acc 0.9950\n",
            "INFO:root:Epoch 0040 | Time 0.475 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9949\n",
            "Epoch 0040 | Time 0.475 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9986 | Test Acc 0.9949\n",
            "INFO:root:Epoch 0041 | Time 0.291 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9961\n",
            "Epoch 0041 | Time 0.291 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0042 | Time 0.301 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9952\n",
            "Epoch 0042 | Time 0.301 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0043 | Time 0.447 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9989 | Test Acc 0.9961\n",
            "Epoch 0043 | Time 0.447 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9989 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0044 | Time 0.292 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9983 | Test Acc 0.9941\n",
            "Epoch 0044 | Time 0.292 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9983 | Test Acc 0.9941\n",
            "INFO:root:Epoch 0045 | Time 0.312 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9958\n",
            "Epoch 0045 | Time 0.312 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0046 | Time 0.370 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9952\n",
            "Epoch 0046 | Time 0.370 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9990 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0047 | Time 0.299 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9944\n",
            "Epoch 0047 | Time 0.299 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9982 | Test Acc 0.9944\n",
            "INFO:root:Epoch 0048 | Time 0.302 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9962\n",
            "Epoch 0048 | Time 0.302 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9962\n",
            "INFO:root:Epoch 0049 | Time 0.291 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9952\n",
            "Epoch 0049 | Time 0.291 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0050 | Time 0.464 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9995 | Test Acc 0.9959\n",
            "Epoch 0050 | Time 0.464 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9995 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0051 | Time 0.305 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9989 | Test Acc 0.9951\n",
            "Epoch 0051 | Time 0.305 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9989 | Test Acc 0.9951\n",
            "INFO:root:Epoch 0052 | Time 0.298 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9954\n",
            "Epoch 0052 | Time 0.298 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9954\n",
            "INFO:root:Epoch 0053 | Time 0.314 (0.103) | NFE-F 25.7 | NFE-B 0.0 | Train Acc 0.9987 | Test Acc 0.9952\n",
            "Epoch 0053 | Time 0.314 (0.103) | NFE-F 25.7 | NFE-B 0.0 | Train Acc 0.9987 | Test Acc 0.9952\n",
            "INFO:root:Epoch 0054 | Time 0.313 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9959\n",
            "Epoch 0054 | Time 0.313 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0055 | Time 0.291 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9961\n",
            "Epoch 0055 | Time 0.291 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0056 | Time 0.298 (0.103) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9947\n",
            "Epoch 0056 | Time 0.298 (0.103) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9988 | Test Acc 0.9947\n",
            "INFO:root:Epoch 0057 | Time 0.494 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9958\n",
            "Epoch 0057 | Time 0.494 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0058 | Time 0.311 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9955\n",
            "Epoch 0058 | Time 0.311 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9994 | Test Acc 0.9955\n",
            "INFO:root:Epoch 0059 | Time 0.309 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9950\n",
            "Epoch 0059 | Time 0.309 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9991 | Test Acc 0.9950\n",
            "INFO:root:Epoch 0060 | Time 0.433 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9957\n",
            "Epoch 0060 | Time 0.433 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9993 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0061 | Time 0.294 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9997 | Test Acc 0.9960\n",
            "Epoch 0061 | Time 0.294 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9997 | Test Acc 0.9960\n",
            "INFO:root:Epoch 0062 | Time 0.294 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9960\n",
            "Epoch 0062 | Time 0.294 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9960\n",
            "INFO:root:Epoch 0063 | Time 0.452 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9961\n",
            "Epoch 0063 | Time 0.452 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0064 | Time 0.296 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9962\n",
            "Epoch 0064 | Time 0.296 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9998 | Test Acc 0.9962\n",
            "INFO:root:Epoch 0065 | Time 0.300 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0065 | Time 0.300 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0066 | Time 0.447 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9962\n",
            "Epoch 0066 | Time 0.447 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9962\n",
            "INFO:root:Epoch 0067 | Time 0.294 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9962\n",
            "Epoch 0067 | Time 0.294 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9962\n",
            "INFO:root:Epoch 0068 | Time 0.293 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0068 | Time 0.293 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0069 | Time 0.315 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0069 | Time 0.315 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0070 | Time 0.491 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "Epoch 0070 | Time 0.491 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0071 | Time 0.306 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9960\n",
            "Epoch 0071 | Time 0.306 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9960\n",
            "INFO:root:Epoch 0072 | Time 0.302 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0072 | Time 0.302 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0073 | Time 0.475 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9960\n",
            "Epoch 0073 | Time 0.475 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9960\n",
            "INFO:root:Epoch 0074 | Time 0.297 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0074 | Time 0.297 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0075 | Time 0.297 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "Epoch 0075 | Time 0.297 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0076 | Time 0.327 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9963\n",
            "Epoch 0076 | Time 0.327 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9963\n",
            "INFO:root:Epoch 0077 | Time 0.297 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0077 | Time 0.297 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0078 | Time 0.296 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0078 | Time 0.296 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0079 | Time 0.300 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0079 | Time 0.300 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0080 | Time 0.442 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "Epoch 0080 | Time 0.442 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9961\n",
            "INFO:root:Epoch 0081 | Time 0.308 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0081 | Time 0.308 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0082 | Time 0.319 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0082 | Time 0.319 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0083 | Time 0.301 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9953\n",
            "Epoch 0083 | Time 0.301 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9953\n",
            "INFO:root:Epoch 0084 | Time 0.489 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0084 | Time 0.489 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0085 | Time 0.298 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "Epoch 0085 | Time 0.298 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "INFO:root:Epoch 0086 | Time 0.314 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0086 | Time 0.314 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0087 | Time 0.292 (0.102) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0087 | Time 0.292 (0.102) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0088 | Time 0.302 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0088 | Time 0.302 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0089 | Time 0.296 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "Epoch 0089 | Time 0.296 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0090 | Time 0.399 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0090 | Time 0.399 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0091 | Time 0.474 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "Epoch 0091 | Time 0.474 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0092 | Time 0.296 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "Epoch 0092 | Time 0.296 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0093 | Time 0.301 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0093 | Time 0.301 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0094 | Time 0.313 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "Epoch 0094 | Time 0.313 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "INFO:root:Epoch 0095 | Time 0.306 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0095 | Time 0.306 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0096 | Time 0.297 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0096 | Time 0.297 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0097 | Time 0.327 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "Epoch 0097 | Time 0.327 (0.105) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "INFO:root:Epoch 0098 | Time 0.445 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9953\n",
            "Epoch 0098 | Time 0.445 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9953\n",
            "INFO:root:Epoch 0099 | Time 0.300 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0099 | Time 0.300 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0100 | Time 0.304 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0100 | Time 0.304 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0101 | Time 0.494 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0101 | Time 0.494 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0102 | Time 0.305 (0.118) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0102 | Time 0.305 (0.118) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0103 | Time 0.313 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "Epoch 0103 | Time 0.313 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0104 | Time 0.452 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "Epoch 0104 | Time 0.452 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9959\n",
            "INFO:root:Epoch 0105 | Time 0.337 (0.118) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0105 | Time 0.337 (0.118) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0106 | Time 0.461 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0106 | Time 0.461 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0107 | Time 0.310 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0107 | Time 0.310 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0108 | Time 0.293 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0108 | Time 0.293 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0109 | Time 0.341 (0.122) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0109 | Time 0.341 (0.122) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0110 | Time 0.311 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0110 | Time 0.311 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0111 | Time 0.472 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0111 | Time 0.472 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0112 | Time 0.295 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0112 | Time 0.295 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0113 | Time 0.318 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0113 | Time 0.318 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0114 | Time 0.491 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0114 | Time 0.491 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0115 | Time 0.313 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "Epoch 0115 | Time 0.313 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9955\n",
            "INFO:root:Epoch 0116 | Time 0.323 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0116 | Time 0.323 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0117 | Time 0.493 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0117 | Time 0.493 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0118 | Time 0.312 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0118 | Time 0.312 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0119 | Time 0.450 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0119 | Time 0.450 (0.106) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0120 | Time 0.313 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0120 | Time 0.313 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0121 | Time 0.312 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0121 | Time 0.312 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0122 | Time 0.470 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0122 | Time 0.470 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0123 | Time 0.309 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0123 | Time 0.309 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0124 | Time 0.309 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0124 | Time 0.309 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0125 | Time 0.448 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0125 | Time 0.448 (0.109) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0126 | Time 0.313 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0126 | Time 0.313 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0127 | Time 0.334 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0127 | Time 0.334 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0128 | Time 0.480 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0128 | Time 0.480 (0.107) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0129 | Time 0.302 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0129 | Time 0.302 (0.112) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0130 | Time 0.329 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0130 | Time 0.329 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0131 | Time 0.527 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "Epoch 0131 | Time 0.527 (0.114) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9957\n",
            "INFO:root:Epoch 0132 | Time 0.310 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0132 | Time 0.310 (0.110) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0133 | Time 0.423 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0133 | Time 0.423 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0134 | Time 0.303 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "Epoch 0134 | Time 0.303 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9956\n",
            "INFO:root:Epoch 0135 | Time 0.308 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0135 | Time 0.308 (0.111) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0136 | Time 0.318 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0136 | Time 0.318 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0137 | Time 0.304 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0137 | Time 0.304 (0.113) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0138 | Time 0.312 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0138 | Time 0.312 (0.108) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "INFO:root:Epoch 0139 | Time 0.316 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n",
            "Epoch 0139 | Time 0.316 (0.104) | NFE-F 26.2 | NFE-B 0.0 | Train Acc 0.9999 | Test Acc 0.9958\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5e13149fc6d>\u001b[0m in \u001b[0;36m<cell line: 278>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a5e13149fc6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modefunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mt_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0my_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0minterp_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interp_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon_step_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_step_index\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_interp_fit\u001b[0;34m(self, y0, y1, k, dt)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n",
        "parser.add_argument('--tol', type=float, default=1e-3)\n",
        "parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n",
        "parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n",
        "parser.add_argument('--nepochs', type=int, default=160)\n",
        "parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n",
        "parser.add_argument('--lr', type=float, default=0.1)\n",
        "parser.add_argument('--batch_size', type=int, default=128)\n",
        "parser.add_argument('--test_batch_size', type=int, default=1000)\n",
        "\n",
        "parser.add_argument('--save', type=str, default='./experiment1')\n",
        "parser.add_argument('--debug', action='store_true')\n",
        "parser.add_argument('--gpu', type=int, default=0)\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "if args.adjoint:\n",
        "    from torchdiffeq import odeint_adjoint as odeint\n",
        "else:\n",
        "    from torchdiffeq import odeint\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "def norm(dim):\n",
        "    return nn.GroupNorm(min(32, dim), dim)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.norm1 = norm(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.norm2 = norm(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "\n",
        "        out = self.relu(self.norm1(x))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(out)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + shortcut\n",
        "\n",
        "\n",
        "class ConcatConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
        "        super(ConcatConv2d, self).__init__()\n",
        "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
        "        self._layer = module(\n",
        "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
        "        ttx = torch.cat([tt, x], 1)\n",
        "        return self._layer(ttx)\n",
        "\n",
        "\n",
        "class ODEfunc(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = norm(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
        "        self.norm2 = norm(dim)\n",
        "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
        "        self.norm3 = norm(dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(t, out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(t, out)\n",
        "        out = self.norm3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n",
        "        return out[1]\n",
        "\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
        "        return x.view(-1, shape)\n",
        "\n",
        "\n",
        "class RunningAverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, momentum=0.99):\n",
        "        self.momentum = momentum\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = None\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        if self.val is None:\n",
        "            self.avg = val\n",
        "        else:\n",
        "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
        "        self.val = val\n",
        "\n",
        "\n",
        "def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n",
        "    if data_aug:\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(28, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n",
        "        shuffle=True, num_workers=2, drop_last=True\n",
        "    )\n",
        "\n",
        "    train_eval_loader = DataLoader(\n",
        "        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n",
        "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n",
        "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_eval_loader\n",
        "\n",
        "\n",
        "def inf_generator(iterable):\n",
        "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
        "        for i, (x, y) in enumerate(inf_generator(train_loader)):\n",
        "    \"\"\"\n",
        "    iterator = iterable.__iter__()\n",
        "    while True:\n",
        "        try:\n",
        "            yield iterator.__next__()\n",
        "        except StopIteration:\n",
        "            iterator = iterable.__iter__()\n",
        "\n",
        "\n",
        "def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n",
        "    initial_learning_rate = args.lr * batch_size / batch_denom\n",
        "\n",
        "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
        "    vals = [initial_learning_rate * decay for decay in decay_rates]\n",
        "\n",
        "    def learning_rate_fn(itr):\n",
        "        lt = [itr < b for b in boundaries] + [True]\n",
        "        i = np.argmax(lt)\n",
        "        return vals[i]\n",
        "\n",
        "    return learning_rate_fn\n",
        "\n",
        "\n",
        "def one_hot(x, K):\n",
        "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
        "\n",
        "\n",
        "def accuracy(model, dataset_loader):\n",
        "    total_correct = 0\n",
        "    for x, y in dataset_loader:\n",
        "        x = x.to(device)\n",
        "        y = one_hot(np.array(y.numpy()), 10)\n",
        "\n",
        "        target_class = np.argmax(y, axis=1)\n",
        "        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n",
        "        total_correct += np.sum(predicted_class == target_class)\n",
        "    return total_correct / len(dataset_loader.dataset)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def makedirs(dirname):\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "\n",
        "\n",
        "def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n",
        "    logger = logging.getLogger()\n",
        "    if debug:\n",
        "        level = logging.DEBUG\n",
        "    else:\n",
        "        level = logging.INFO\n",
        "    logger.setLevel(level)\n",
        "    if saving:\n",
        "        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n",
        "        info_file_handler.setLevel(level)\n",
        "        logger.addHandler(info_file_handler)\n",
        "    if displaying:\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setLevel(level)\n",
        "        logger.addHandler(console_handler)\n",
        "    logger.info(filepath)\n",
        "    with open(filepath, \"r\") as f:\n",
        "        logger.info(f.read())\n",
        "\n",
        "    for f in package_files:\n",
        "        logger.info(f)\n",
        "        with open(f, \"r\") as package_f:\n",
        "            logger.info(package_f.read())\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    makedirs(args.save)\n",
        "    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath('/content/drive/MyDrive/Colab Notebooks/cs376.ipynb'))\n",
        "    logger.info(args)\n",
        "\n",
        "    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    is_odenet = args.network == 'odenet'\n",
        "\n",
        "    if args.downsampling_method == 'conv':\n",
        "        downsampling_layers = [\n",
        "            nn.Conv2d(1, 64, 3, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "        ]\n",
        "    elif args.downsampling_method == 'res':\n",
        "        downsampling_layers = [\n",
        "            nn.Conv2d(1, 64, 3, 1),\n",
        "            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
        "        ]\n",
        "\n",
        "    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n",
        "    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n",
        "\n",
        "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n",
        "\n",
        "    logger.info(model)\n",
        "    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n",
        "        args.data_aug, args.batch_size, args.test_batch_size\n",
        "    )\n",
        "\n",
        "    data_gen = inf_generator(train_loader)\n",
        "    batches_per_epoch = len(train_loader)\n",
        "\n",
        "    lr_fn = learning_rate_with_decay(\n",
        "        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n",
        "        decay_rates=[1, 0.1, 0.01, 0.001]\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
        "\n",
        "    best_acc = 0\n",
        "    batch_time_meter = RunningAverageMeter()\n",
        "    f_nfe_meter = RunningAverageMeter()\n",
        "    b_nfe_meter = RunningAverageMeter()\n",
        "    end = time.time()\n",
        "\n",
        "    for itr in range(args.nepochs * batches_per_epoch):\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr_fn(itr)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x, y = data_gen.__next__()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        if is_odenet:\n",
        "            nfe_forward = feature_layers[0].nfe\n",
        "            feature_layers[0].nfe = 0\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if is_odenet:\n",
        "            nfe_backward = feature_layers[0].nfe\n",
        "            feature_layers[0].nfe = 0\n",
        "\n",
        "        batch_time_meter.update(time.time() - end)\n",
        "        if is_odenet:\n",
        "            f_nfe_meter.update(nfe_forward)\n",
        "            b_nfe_meter.update(nfe_backward)\n",
        "        end = time.time()\n",
        "\n",
        "        if itr % batches_per_epoch == 0:\n",
        "            with torch.no_grad():\n",
        "                train_acc = accuracy(model, train_eval_loader)\n",
        "                val_acc = accuracy(model, test_loader)\n",
        "                if val_acc > best_acc:\n",
        "                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n",
        "                    best_acc = val_acc\n",
        "                logger.info(\n",
        "                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n",
        "                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n",
        "                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n",
        "                        b_nfe_meter.avg, train_acc, val_acc\n",
        "                    )\n",
        "                )"
      ]
    }
  ]
}